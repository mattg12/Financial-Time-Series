{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"../assets/grt_logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Garton Research & Trading\n",
    "\n",
    "_August 16, 2019_\n",
    "\n",
    "---\n",
    "\n",
    "## Forecasting Futures Prices with LSTMs\n",
    "\n",
    "---\n",
    "\n",
    "**Context:** The purpose of this study is to experiment with implementing Long Short Term Memory neural networks to predict futures prices. The data for this study consists of continuous futures contracts (generic first nearby with naive rolling method) from the Wiki Continuous Futures dataset on [Quandl](https://www.quandl.com/data/CHRIS-Wiki-Continuous-Futures). The specific markets I chose to use are based on the markets used in the original 'Turtle Traders' strategy (CITATION NEEDED).\n",
    "\n",
    "_Author: Matthew Garton_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# necessary for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a dataset\n",
    "sugar = pd.read_csv('../data/ICE_SB1.csv', \n",
    "                    usecols=['Date','Settle'], \n",
    "                    index_col='Date', \n",
    "                    parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar.plot(figsize=(10,7))\n",
    "plt.title('Sugar Prices from 1961 to Present', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on regimes and sampling\n",
    "\n",
    "It is clear that the full history here is dominated by a few significant regimes. My expectation is that the LSTM model will be trained on a relatively short time span to predict an even shorter one, so looking at several decades of data which include some extreme events is unlikely to be successful. So I will choose a more recent subsample of the data to look at. Starting in the year 2000, in the middle of a 'boring' regime may be reasonable. I'll have to return to this later, but for now I'm satisfied with ignoring the extreme inflation of the 70's and 80's (with the knowledge that my strategy would become invalid if such a regime were to reappear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar = sugar['2000':]\n",
    "sugar.plot(figsize=(10,7))\n",
    "plt.title('Sugar Prices from 2000 to Present', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference the data until approximately stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(sugar['Settle'].diff())\n",
    "plt.title('First-differencing of sugar prices', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above appears roughly stationary based on a quick 'eyeball' test - it looks like a White Noise process. However, I want to define stationarity with more rigor, as well as have the functionality to algorithmically transform an arbitrary time series into a stationary one. Below I define two functions which should be applicable to any time series analysis:\n",
    "\n",
    "* A function to difference the data until the (differenced) series 'passes' an augmented Dickey-Fuller test. This function will be called on the dataset before any modeling or other preprocessing.\n",
    "* A function to invert the differencing that was necessary to achieve stationarity. This function will be called at the end of the modeling process, when I want to view my predictions in the original units of the time series.\n",
    "\n",
    "**Note:** These functions only work for integer differencing. Generalizing to allow fractional differencing is something I hope to return to in a future iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stationary(ts, max_diff=3, p_value=0.05):\n",
    "    '''\n",
    "    Returns time series with d-order differencing,\n",
    "    where d is the smallest number of differencing that\n",
    "    achieves stationarity based on an augmented Dicker-Fuller test\n",
    "    Inputs:\n",
    "    ts - pandas Series with datetime index\n",
    "    dvals - range of d values to search over\n",
    "    pval - pvalue cutoff to use for ADF test\n",
    "    Outputs:\n",
    "    ts_d - differenced time series\n",
    "    '''\n",
    "    ts_d = ts.copy()\n",
    "    startvals = []\n",
    "    for d in range(max_diff+1):\n",
    "        startvals.append(ts_d.dropna()[0])\n",
    "        ts_d = ts_d.diff()\n",
    "        pval = adfuller(ts_d.dropna())[1]\n",
    "        if pval < p_value:\n",
    "            print(f'Order of differencing: {d+1}')\n",
    "            return ts_d, startvals\n",
    "        else:\n",
    "            continue\n",
    "    print('Cannot achieve stationarity. Review data or try a wider range.')\n",
    "    \n",
    "def invert_differencing(startvals, ts_diff):\n",
    "    '''\n",
    "    Inverse transform of time series differencing\n",
    "    to be applied to output from predictive model\n",
    "    Inputs:\n",
    "    ts_diff - d-differenced time series data\n",
    "    d - order of differencing\n",
    "    '''\n",
    "    ts = ts_diff.copy()\n",
    "    while startvals:\n",
    "        for i in startvals[::-1]:\n",
    "            ts.iloc[len(startvals)-1] = i\n",
    "            startvals.pop()\n",
    "            ts = ts.cumsum()\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data for LSTM\n",
    "\n",
    "LSTM neural networks require input in a particular shape. I'll demonstrate two methods for preparing the data: one will be a more 'hands on' approach that is more explicit but requires more steps. Then, I'll make use of the `TimeSeriesGenerator` provided by `keras`. `TimeSeriesGenerator` is a time saver, but I prefer starting out with the manual approach, as data prep for neural nets can get convoluted (no pun intended) and it helps to be forced to keep track of the state of your data at each stage. This is especially crucial in time series forecasting, which typically involves transformations on the front-end that will have to be inverted at the end (i.e. differencing for stationarity).\n",
    "\n",
    "**Citation:** Credit to Jason Brownlee, PhD and his blog [Machine Learning Mastery](https://machinelearningmastery.com/) which was an instrumental resource for me in learning LSTM workflows. I've endeavored to write my own functions where possible, but the code snippets in his blog were a great starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual approach - define all functions I'll need\n",
    "def generate_sequences(data, lags=7):\n",
    "    '''\n",
    "    Returns a time series that has been formatted for a supervised learning problem\n",
    "    \n",
    "    Inputs:\n",
    "    data - time series data as a pandas Series\n",
    "    \n",
    "    Returns:\n",
    "    df* - dataframe where each row includes columns of lagged observations\n",
    "    *returns the values of this df as a numpy array\n",
    "    '''    \n",
    "    columns = [data.shift(i) for i in range(1, lags+1)]\n",
    "    columns.append(data)\n",
    "    data = pd.concat(columns, axis=1)\n",
    "    data.fillna(0, inplace=True)\n",
    "    return data.values\n",
    "\n",
    "def scale_data(train, test):\n",
    "    '''\n",
    "    MinMaxScale data before fitting LSTM\n",
    "    Inputs:\n",
    "    train, test - training and testing data\n",
    "    Outputs:\n",
    "    scaler - MinMaxScaler object fit to training data\n",
    "    train_sc, test_sc - scaled training and testing data\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train = train.reshape(-1, 1)\n",
    "    train_sc = scaler.fit_transform(train)\n",
    "    \n",
    "    #test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test = test.reshape(-1, 1)\n",
    "    test_sc = scaler.transform(test)\n",
    "    return scaler, train_sc, test_sc\n",
    "\n",
    "def lstm_pipe(ts, train_size=0.7, lag_windows=7):\n",
    "    \n",
    "    # difference for stationarity\n",
    "    data, startvals = make_stationary(ts)\n",
    "    \n",
    "    # train test split\n",
    "    train_split = int(train_size*data.shape[0])\n",
    "    train, test = data[0:train_split], data[train_split:data.shape[0]]\n",
    "    \n",
    "    # scale data\n",
    "    scaler, train_sc, test_sc = scale_data(train.values, test.values)\n",
    "    \n",
    "    # convert to sequences\n",
    "    train_samples = generate_sequences(train_sc, lags=lag_windows)\n",
    "    test_samples = generate_sequences(test_sc, lags=lag_windows)\n",
    "    \n",
    "    # set up X and y for training\n",
    "    X, y = train_samples[:, 0:-1], train_samples[:, -1]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # set up X and y for test data\n",
    "    Xt, yt = test_samples[:, 0:-1], test_samples[:, -1]\n",
    "    Xt = Xt.reshape(Xt.shape[0], 1, Xt.shape[1])\n",
    "    \n",
    "    return X, y, Xt, yt, startvals, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Pipeline\n",
    "\n",
    "**Front-end:**\n",
    "1. Difference the data for stationarity\n",
    "2. Train-test Split\n",
    "3. Scale the data --> MinMaxScaler in this case\n",
    "4. Reshape data --> Convert to observations of sequences\n",
    "\n",
    "**Back-end:**\n",
    "1. Inverse scale\n",
    "2. Inverse Difference\n",
    "3. Re-combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, Xt, yt, startvals, scaler = lstm_pipe(sugar['Settle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, X, y, Xt, yt, epochs=1, batch_size=1):\n",
    "    '''\n",
    "    Train the LSTM model and validate on testing data\n",
    "    Inputs:\n",
    "    model - compiled LSTM model\n",
    "    X - training inputs\n",
    "    y - training outputs\n",
    "    Xt - validation inputs\n",
    "    yt - validation outputs\n",
    "    epochs - number of training epochs\n",
    "    Outputs:\n",
    "    model - LSTM model fit to training data\n",
    "    history - model history\n",
    "    '''\n",
    "    \n",
    "    # instantiate empty dictionary for model history\n",
    "    model_hist = {\n",
    "        'loss': [],\n",
    "        'val_loss': []\n",
    "           }\n",
    "    \n",
    "    # reset the state of the model between each training epoch\n",
    "    for i in range(epochs):\n",
    "        if i < epochs-1:\n",
    "            history = model.fit(X, y, \n",
    "                                epochs=1, \n",
    "                                batch_size=batch_size,\n",
    "                                verbose=0, \n",
    "                                shuffle=False,\n",
    "                                validation_data=(Xt, yt))\n",
    "            model_hist['loss'].extend(history.history['loss'])\n",
    "            model_hist['val_loss'].extend(history.history['val_loss'])\n",
    "            model.reset_states()\n",
    "        # return the history after the last training epoch; validate on testing data\n",
    "        else:\n",
    "            history = model.fit(X, y, \n",
    "                                epochs=1, \n",
    "                                batch_size=batch_size, \n",
    "                                verbose=1, \n",
    "                                shuffle=False,\n",
    "                                validation_data=(Xt, yt))\n",
    "            model_hist['loss'].extend(history.history['loss'])\n",
    "            model_hist['val_loss'].extend(history.history['val_loss'])\n",
    "            model.reset_states()\n",
    "            \n",
    "    return model, model_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters here\n",
    "neurons = 100\n",
    "batch_size =  1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture and build\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, \n",
    "               activation='tanh',\n",
    "               batch_input_shape=(batch_size, X.shape[1], X.shape[2]),\n",
    "               stateful=True))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model, model_hist = train_lstm(model, X, y, Xt, yt, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(model_hist['loss'], label='Train loss')\n",
    "plt.plot(model_hist['val_loss'], label='Test loss')\n",
    "plt.title('Results from LSTM model', fontsize=18)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting with the model\n",
    "\n",
    "**Note:** to facilitate comparing predictions to actual data, I will need to reverse the transformations that I originally made to my data, so that I can ultimately plot 'prices' and 'predicted prices.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model.predict(X, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(Xt, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse transform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_lstm_pipe(preds, scaler, startvals):\n",
    "    # inverse scale\n",
    "    ts_diff = scaler.inverse_transform(preds)\n",
    "    \n",
    "    # inverse difference\n",
    "    ts = invert_differencing(startvals, ts_diff)\n",
    "    \n",
    "    return ts_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inverse_lstm_pipe(yhat, scaler, startvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(Xt[0])\n",
    "plt.plot(yt)\n",
    "plt.plot(preds)\n",
    "plt.title('LSTM Forecast', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The model does not seem to be working. I need to continue working on the transforms and inverse transforms and make sure they're working as I would expect them to work. Once I can get my predictions back into the original units, it will be easier to plot predictions versus test data and see if I'm on the right track."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:quant] *",
   "language": "python",
   "name": "conda-env-quant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
