{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"../assets/grt_logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Garton Research & Trading\n",
    "\n",
    "_August 23, 2019_\n",
    "\n",
    "---\n",
    "\n",
    "## Forecasting Futures Prices with LSTMs (NumPy)\n",
    "\n",
    "---\n",
    "\n",
    "**Context:** The purpose of this study is to experiment with implementing Long Short Term Memory neural networks to predict futures prices. The data for this study consists of continuous futures contracts (generic first nearby with naive rolling method) from the Wiki Continuous Futures dataset on [Quandl](https://www.quandl.com/data/CHRIS-Wiki-Continuous-Futures). The specific markets I chose to use are based on the markets used in the original ['Turtle Traders'](https://bigpicture.typepad.com/comments/files/turtlerules.pdf) strategy.\n",
    "\n",
    "This notebook is a derivation of my initial work [Forecasting Futures Prices with LSTMs](https://nbviewer.jupyter.org/github/mattg12/Financial-Time-Series/blob/master/code/Forecasting%20Futures%20Prices%20with%20LSTMs.ipynb) in which I am trying to replicate the data wrangling and modeling without using `pandas`. Since `keras` actually works with `numpy` arrays, it would be more straightforward if all data preprocessing could be done in `numpy`, cutting out the 'middle man', so to speak. I found in my initial approach that having to transfer my data back and forth between `numpy` arrays and `pandas` dataframes felt inefficient and inelegant.\n",
    "\n",
    "_Author: Matthew Garton_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard data science imports - no pandas!\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# necessary for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_data(data, train_split=0.7):\n",
    "    # difference data\n",
    "    diff = np.diff(data)\n",
    "    # train test split\n",
    "    train, test = diff[:int(train_split*diff.shape[0])], diff[int(train_split*diff.shape[0]):]\n",
    "    train, test = train.reshape(-1, 1), test.reshape(-1, 1)\n",
    "    # minmax scale data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test, scaler\n",
    "\n",
    "def inverse_transform_data(predictions, history, scaler):\n",
    "    # invert scaling\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    # inverse differencing\n",
    "    predictions = np.r_[history[0].reshape(-1,1), predictions].cumsum()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "sugar = np.genfromtxt('../data/ICE_SB1.csv', delimiter=',', skip_header=1, usecols=4, dtype=float)\n",
    "\n",
    "# preprocess data\n",
    "train, test, scaler = tranform_data(sugar)\n",
    "\n",
    "# generate samples\n",
    "lags=7\n",
    "batch_size=1\n",
    "neurons=100\n",
    "train_samples = TimeseriesGenerator(train, train, length=lags, batch_size=batch_size)\n",
    "test_samples = TimeseriesGenerator(test, test, length=lags, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mattg/anaconda3/envs/quant/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define model architecture and build\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, \n",
    "               activation='tanh',\n",
    "               batch_input_shape=(batch_size, lags, 1),\n",
    "               stateful=True))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, train, epochs=1, batch_size=1):\n",
    "    # reset the state of the model between each training epoch\n",
    "    for i in range(epochs):\n",
    "        history = model.fit_generator(train)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 50s 5ms/step - loss: 0.0055\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 49s 5ms/step - loss: 0.0055\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 49s 5ms/step - loss: 0.0054\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 45s 5ms/step - loss: 0.0054\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 46s 5ms/step - loss: 0.0054\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 46s 5ms/step - loss: 0.0055\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 49s 5ms/step - loss: 0.0054\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 49s 5ms/step - loss: 0.0055\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 47s 5ms/step - loss: 0.0054\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 46s 5ms/step - loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(model, train_samples, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005049479543018256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(data, history, model, scaler):\n",
    "    yhat = model.predict_generator(data)\n",
    "    forecast = inverse_transform_data(yhat, history, scaler)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = make_forecast(test_samples, sugar, model, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Implementing the model fully in `numpy` was easier than anticipated, and `TimeSeriesGenerator` appears to be avery powerful tool. I will first clean up this code and test my understanding to be sure I am setting everything up correctly, then I will take next steps in improving my model.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Make predictions with the model, and inverse-transform those predictions to evaluate against actual data.\n",
    "2. Explore parameters of both TimseriesGenerator and LSTM model to understand what my options are to tune the model.\n",
    "3. Work on tweaking and optimizing the model on simple datasets to get a better understanding of how changes to model architecture affect it's ability to learn from sequential data.\n",
    "4. Play around with longer lookbacks and prediction windows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:quant] *",
   "language": "python",
   "name": "conda-env-quant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
