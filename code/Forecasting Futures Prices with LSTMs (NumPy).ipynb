{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"../assets/grt_logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Garton Research & Trading\n",
    "\n",
    "_August 23, 2019_\n",
    "\n",
    "---\n",
    "\n",
    "## Forecasting Futures Prices with LSTMs (NumPy)\n",
    "\n",
    "---\n",
    "\n",
    "**Context:** The purpose of this study is to experiment with implementing Long Short Term Memory neural networks to predict futures prices. The data for this study consists of continuous futures contracts (generic first nearby with naive rolling method) from the Wiki Continuous Futures dataset on [Quandl](https://www.quandl.com/data/CHRIS-Wiki-Continuous-Futures). The specific markets I chose to use are based on the markets used in the original ['Turtle Traders'](https://bigpicture.typepad.com/comments/files/turtlerules.pdf) strategy.\n",
    "\n",
    "This notebook is a derivation of my initial work [Forecasting Futures Prices with LSTMs](https://nbviewer.jupyter.org/github/mattg12/Financial-Time-Series/blob/master/code/Forecasting%20Futures%20Prices%20with%20LSTMs.ipynb) in which I am trying to replicate the data wrangling and modeling without using `pandas`. Since `keras` actually works with `numpy` arrays, it would be more straightforward if all data preprocessing could be done in `numpy`, cutting out the 'middle man', so to speak. I found in my initial approach that having to transfer my data back and forth between `numpy` arrays and `pandas` dataframes felt inefficient and inelegant.\n",
    "\n",
    "_Author: Matthew Garton_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard data science imports - no pandas!\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# necessary for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "sugar = np.genfromtxt('../data/ICE_SB1.csv', delimiter=',', skip_header=1, usecols=4, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference data\n",
    "s_diff = np.diff(sugar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_split = 0.7\n",
    "train, test = s_diff[:int(train_split*s_diff.shape[0])], s_diff[int(train_split*s_diff.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scale\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train = train.reshape(-1, 1)\n",
    "train_sc = scaler.fit_transform(train)\n",
    "test = test.reshape(-1, 1)\n",
    "test_sc = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "lags=7\n",
    "batch_size=1\n",
    "neurons=100\n",
    "train_samples = TimeseriesGenerator(train, train, length=lags, batch_size=batch_size)\n",
    "test_samples = TimeseriesGenerator(test, test, length=lags, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture and build\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, \n",
    "               activation='tanh',\n",
    "               batch_input_shape=(batch_size, lags, 1),\n",
    "               stateful=True))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mattg/anaconda3/envs/quant/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "9781/9781 [==============================] - 60s 6ms/step - loss: 0.1530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ba9e7aef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1405682139916179"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Implementing the model fully in `numpy` was easier than anticipated, and `TimeSeriesGenerator` appears to be avery powerful tool. I will first clean up this code and test my understanding to be sure I am setting everything up correctly, then I will take next steps in improving my model.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Make predictions with the model, and inverse-transform those predictions to evaluate against actual data.\n",
    "2. Explore parameters of both TimseriesGenerator and LSTM model to understand what my options are to tune the model.\n",
    "3. Work on tweaking and optimizing the model on simple datasets to get a better understanding of how changes to model architecture affect it's ability to learn from sequential data.\n",
    "4. Play around with longer lookbacks and prediction windows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:quant] *",
   "language": "python",
   "name": "conda-env-quant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
